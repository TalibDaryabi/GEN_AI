{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install peft\n",
    "!pip install --upgrade ipywidgets jupyter notebook"
   ],
   "id": "44896c28ae3bf220"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-06T13:55:09.737131Z",
     "start_time": "2025-03-06T13:55:09.731608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding, TrainingArguments , Trainer\n",
    ")\n",
    "from peft import PeftConfig, PeftModel, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T14:13:04.663831Z",
     "start_time": "2025-03-06T14:13:04.660638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ],
   "id": "c26ba09016fefe5d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T14:52:58.828907Z",
     "start_time": "2025-03-06T14:52:53.445701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ],
   "id": "b4523f15473c0204",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "6fbe147c6f8e2977"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T14:56:55.170069Z",
     "start_time": "2025-03-06T14:56:55.116952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "train_val_split = imdb_dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "imdb_dataset = {\n",
    "    \"train\": train_val_split[\"train\"],\n",
    "    \"validation\": train_val_split[\"test\"],\n",
    "    \"test\": imdb_dataset[\"test\"]\n",
    "}\n",
    "imdb_dataset"
   ],
   "id": "ba2fe53181b0650e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 21250\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 3750\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " })}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:13.077754Z",
     "start_time": "2025-03-06T15:14:12.908790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_dataset[\"train\"] = imdb_dataset[\"train\"].shuffle().select(range(300))\n",
    "imdb_dataset[\"test\"] = imdb_dataset[\"test\"].shuffle().select(range(100))\n",
    "imdb_dataset[\"validation\"] = imdb_dataset[\"validation\"].shuffle().select(range(100))\n",
    "imdb_dataset[\"train\"]\n"
   ],
   "id": "b488032744f3e706",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T14:00:14.096544Z",
     "start_time": "2025-03-06T14:00:14.092010Z"
    }
   },
   "cell_type": "code",
   "source": "imdb_test",
   "id": "331bc15f668c7138",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T14:57:33.656021Z",
     "start_time": "2025-03-06T14:57:33.648408Z"
    }
   },
   "cell_type": "code",
   "source": "imdb_dataset",
   "id": "36526bfc475e092c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 3750\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 100\n",
       " })}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:17.410695Z",
     "start_time": "2025-03-06T15:14:16.604885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2 ,id2label=id2label,\n",
    "    label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"text\"]\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "# Apply tokenization to each split\n",
    "tokenized_dataset = {\n",
    "    \"train\": imdb_dataset[\"train\"].map(tokenize_function, batched=True, batch_size=32),\n",
    "    \"validation\": imdb_dataset[\"validation\"].map(tokenize_function, batched=True, batch_size=32),\n",
    "    \"test\": imdb_dataset[\"test\"].map(tokenize_function, batched=True, batch_size=32)\n",
    "}\n",
    "\n",
    "tokenized_dataset"
   ],
   "id": "6b6374e05da284b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 2072.97 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1464.68 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1294.83 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 100\n",
       " })}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:21.189585Z",
     "start_time": "2025-03-06T15:14:21.181607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_dataset[('train')].remove_columns([\"text\"])\n",
    "tokenized_dataset[('validation')].remove_columns([\"text\"])\n",
    "tokenized_dataset[('test')].remove_columns([\"text\"])\n",
    "#tokenized_dataset = {split: dataset.remove_columns([\"text\"]) for split, dataset in tokenized_dataset.items()}\n"
   ],
   "id": "5558cc2005a85b64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation Metrics",
   "id": "1ee11bc85abaeed0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:25.690724Z",
     "start_time": "2025-03-06T15:14:24.258980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"binary\")[\"f1\"]\n",
    "    }"
   ],
   "id": "d1984495b7ecc4c6",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### testing the untrained model",
   "id": "dd628bc1e75771cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:28.068791Z",
     "start_time": "2025-03-06T15:14:27.916308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "for text in text_list:\n",
    "    tokens = tokenizer(text , return_tensors=\"pt\")\n",
    "    logit = model(**tokens).logits\n",
    "    print(\"logit: \", logit)\n",
    "    predictions = torch.argmax(logit, dim=1)\n",
    "    predicted_label = id2label[predictions.item()]\n",
    "\n",
    "    print(f\"{text} - {predicted_label}\")"
   ],
   "id": "b8b584a0bd42c349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit:  tensor([[0.0724, 0.0737]], grad_fn=<AddmmBackward0>)\n",
      "It was good. - Positive\n",
      "logit:  tensor([[0.0656, 0.0812]], grad_fn=<AddmmBackward0>)\n",
      "Not a fan, don't recommed. - Positive\n",
      "logit:  tensor([[0.0946, 0.0833]], grad_fn=<AddmmBackward0>)\n",
      "Better than the first one. - Negative\n",
      "logit:  tensor([[0.0766, 0.0968]], grad_fn=<AddmmBackward0>)\n",
      "This is not worth watching even once. - Positive\n",
      "logit:  tensor([[0.0917, 0.0947]], grad_fn=<AddmmBackward0>)\n",
      "This one is a pass. - Positive\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train Model",
   "id": "648d94ca3452421e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:34.097263Z",
     "start_time": "2025-03-06T15:14:34.091071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                        r=4,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules = ['q_lin'])\n",
    "peft_config"
   ],
   "id": "28a86235a0473c2b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=4, target_modules={'q_lin'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:14:37.210318Z",
     "start_time": "2025-03-06T15:14:37.186304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "45ed2833774ddb6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:20:49.450237Z",
     "start_time": "2025-03-06T15:14:39.476421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import get_scheduler, AutoProcessor\n",
    "from torch.optim import AdamW\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=checkpoint + \"-lora-text-classification\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,   # Optional: applies L2 regularization\n",
    "    warmup_ratio=0,  # Keep warmup steps at 0\n",
    "    # logging_steps=10,  # Logging frequency\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "num_training_steps = (len(tokenized_dataset[\"train\"]) // training_args.per_device_train_batch_size) * training_args.num_train_epochs\n",
    "# optimizer = SGD(model.parameters(), lr=5e-3, momentum=0.9)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    "\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# create trainer object\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    processing_class=processor,  # Use processing_class instead of tokenizer\n",
    "    optimizers=(optimizer, lr_scheduler),  # Pass manually defined optimizer & scheduler\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ],
   "id": "e501453d9e087023",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/30 04:34 < 16:45, 0.02 it/s, Epoch 0.70/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 47\u001B[0m\n\u001B[0;32m     35\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     36\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     37\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[0;32m     44\u001B[0m )\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# train model\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\transformers\\trainer.py:2171\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2169\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[0;32m   2172\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[0;32m   2173\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[0;32m   2174\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[0;32m   2175\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[0;32m   2176\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\transformers\\trainer.py:2531\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2524\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2525\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2526\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2527\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2528\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2529\u001B[0m )\n\u001B[0;32m   2530\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2531\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[0;32m   2533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2534\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2535\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2536\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2537\u001B[0m ):\n\u001B[0;32m   2538\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2539\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\transformers\\trainer.py:3712\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3709\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_accepts_loss_kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3710\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n\u001B[1;32m-> 3712\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mbackward(loss, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   3714\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\accelerate\\accelerator.py:2246\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2244\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[0;32m   2245\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2246\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    625\u001B[0m     )\n\u001B[1;32m--> 626\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    628\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[0;32m    348\u001B[0m     tensors,\n\u001B[0;32m    349\u001B[0m     grad_tensors_,\n\u001B[0;32m    350\u001B[0m     retain_graph,\n\u001B[0;32m    351\u001B[0m     create_graph,\n\u001B[0;32m    352\u001B[0m     inputs,\n\u001B[0;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    355\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Transformer\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    824\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    825\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n",
    "for text in text_list:\n",
    "    tokens = tokenizer(text , return_tensors=\"pt\")\n",
    "    logit = model(**tokens).logits\n",
    "    print(\"logit: \", logit)\n",
    "    predictions = torch.argmax(logit, dim=1)\n",
    "    predicted_label = id2label[predictions.item()]\n",
    "\n",
    "    print(f\"{text} - {predicted_label}\")"
   ],
   "id": "63f28f78585f2ff9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OPTIONAL: Push Model to HUb",
   "id": "69dbcefcd8ad0ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# option 1: notebook login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "hf_name = 'talibdaryabi' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + checkpoint + \"-lora-text-classification\" # you can name the model whatever you want\n",
    "model.push_to_hub(model_id) # save model\n",
    "trainer.push_to_hub(model_id)"
   ],
   "id": "d7d5064e27c0f458"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
